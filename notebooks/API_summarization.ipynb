{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1K8Tiep0CFc",
        "outputId": "ce618886-50ff-4970-bad8-b2bae594f906"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install openai -q\n",
        "\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "client = OpenAI(api_key= #edited out to push to GitHub)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = #edited out to push to github"
      ],
      "metadata": {
        "id": "-xw7UrIkD-rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curated_df = pd.read_csv('/content/drive/MyDrive/Dissertation/curated_expanded.csv')\n",
        "pilot_df = pd.read_csv('/content/drive/MyDrive/Dissertation/pilotdata.csv')"
      ],
      "metadata": {
        "id": "ct1f_2rF5CxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompting Conditions"
      ],
      "metadata": {
        "id": "xPgeo-Tp55u-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompting_conditions = {\n",
        "    'default': {\n",
        "        'name': 'Default',\n",
        "        'system': \"You are a helpful assistant that summarizes text.\",\n",
        "        'user': \"Summarize the following deposition testimony:\\n\\n{text}\"\n",
        "    },\n",
        "\n",
        "    'feature_preserving': {\n",
        "        'name': 'Feature-Preserving',\n",
        "        'system': \"You are a summarization assistant that preserves linguistic nuance.\",\n",
        "        'user': \"\"\"Summarize the following deposition testimony.\n",
        "IMPORTANT: Preserve all hedging language (I think, maybe, possibly),\n",
        "disfluencies (um, uh, repetitions), modal expressions (could, might, would),\n",
        "and temporal markers. Do not strip or clean up uncertain language.\n",
        "\n",
        "{text}\"\"\"\n",
        "    },\n",
        "\n",
        "    'legal_context': {\n",
        "        'name': 'Legal-Context',\n",
        "        'system': \"You are a legal document summarization assistant.\",\n",
        "        'user': \"\"\"Summarize this legal deposition excerpt for case review.\n",
        "Maintain all uncertainty markers, hedges, and speech patterns as they may be\n",
        "legally significant for assessing witness credibility and testimony accuracy.\n",
        "\n",
        "{text}\"\"\"\n",
        "    },\n",
        "\n",
        "    'bias_aware': {\n",
        "        'name': 'Bias-Aware',\n",
        "        'system': \"You are a linguistically-aware summarization assistant.\",\n",
        "        'user': \"\"\"Summarize this testimony without imposing standard language norms.\n",
        "Preserve disfluencies, hedges, and uncertain language that may be characteristic\n",
        "of the speaker's communication style, gender, or language background.\n",
        "\n",
        "{text}\"\"\"\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "oXrZLzRu5kPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summarization Function"
      ],
      "metadata": {
        "id": "8nKaKvEX6HKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_gpt_summary(text, condition='default', model='gpt-3.5-turbo'):\n",
        "    \"\"\"\n",
        "    Generate summary using OpenAI API with specified prompting condition\n",
        "\n",
        "    Args:\n",
        "        text: Input text to summarize\n",
        "        condition: One of 'default', 'feature_preserving', 'legal_context', 'bias_aware'\n",
        "        model: 'gpt-3.5-turbo' (cheaper) or 'gpt-4' (better)\n",
        "    \"\"\"\n",
        "\n",
        "    if pd.isna(text) or not text or len(str(text).strip()) < 10:\n",
        "        return \"\"\n",
        "\n",
        "    prompt_config = prompting_conditions[condition]\n",
        "\n",
        "    try:\n",
        "        # New v1.0+ syntax\n",
        "        client = OpenAI(api_key=#edited out to push to github\n",
        "            model=model,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": prompt_config['system']},\n",
        "                {\"role\": \"user\", \"content\": prompt_config['user'].format(text=text)}\n",
        "            ],\n",
        "            max_tokens=150,\n",
        "            temperature=0.3,\n",
        "            top_p=0.95\n",
        "        )\n",
        "\n",
        "        return response.choices[0].message.content\n",
        "\n",
        "    except openai.RateLimitError:\n",
        "        print(\"Rate limit hit, waiting 20 seconds...\")\n",
        "        time.sleep(20)\n",
        "        return generate_gpt_summary(text, condition, model)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"\"\n"
      ],
      "metadata": {
        "id": "b5LeX1lQ78Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing Datasets with all Conditions"
      ],
      "metadata": {
        "id": "QCYcROTN6PG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = 'gpt-3.5-turbo'\n",
        "\n",
        "print(f\"\\n Using model: {MODEL}\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for condition_key, condition_info in prompting_conditions.items():\n",
        "\n",
        "    print(f\"\\nProcessing with {condition_info['name']} Condition\")\n",
        "    print(\"-\"*50)\n",
        "\n",
        "    # CURATED DATASET\n",
        "    print(f\"\\nProcessing curated dataset...\")\n",
        "    curated_summaries = []\n",
        "\n",
        "    for idx, row in tqdm(curated_df.iterrows(), total=len(curated_df),\n",
        "                        desc=f\"Curated-{condition_key}\"):\n",
        "        text = row['Excerpt']\n",
        "        summary = generate_gpt_summary(text, condition_key, MODEL)\n",
        "        curated_summaries.append(summary)\n",
        "\n",
        "        time.sleep(0.5)  # Half second between requests\n",
        "\n",
        "        if idx % 20 == 0 and idx > 0:\n",
        "            curated_df[f'gpt_{condition_key}'] = curated_summaries + [''] * (len(curated_df) - len(curated_summaries))\n",
        "            curated_df.to_csv(f'/content/drive/MyDrive/Dissertation/gpt/curated_gpt_{condition_key}_temp.csv', index=False)\n",
        "\n",
        "    curated_df[f'gpt_{condition_key}'] = curated_summaries\n",
        "\n",
        "    # PILOT DATASET\n",
        "    print(f\"\\nProcessing pilot dataset...\")\n",
        "    pilot_summaries = []\n",
        "\n",
        "    for idx, row in tqdm(pilot_df.iterrows(), total=len(pilot_df),\n",
        "                        desc=f\"Pilot-{condition_key}\"):\n",
        "        text = row['TEXT']\n",
        "        summary = generate_gpt_summary(text, condition_key, MODEL)\n",
        "        pilot_summaries.append(summary)\n",
        "\n",
        "        time.sleep(0.5)\n",
        "\n",
        "        if idx % 20 == 0 and idx > 0:\n",
        "            pilot_df[f'gpt_{condition_key}'] = pilot_summaries + [''] * (len(pilot_df) - len(pilot_summaries))\n",
        "            pilot_df.to_csv(f'/content/drive/MyDrive/Dissertation/gpt/pilot_gpt_{condition_key}_temp.csv', index=False)\n",
        "\n",
        "    pilot_df[f'gpt_{condition_key}'] = pilot_summaries\n",
        "\n",
        "    print(f\"âœ“ Completed {condition_info['name']} condition\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTkvaQIe8CMf",
        "outputId": "5664e0ee-6303-464e-9049-4f17b183db60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ¤– Using model: gpt-3.5-turbo\n",
            "============================================================\n",
            "\n",
            "ðŸ“Š Processing with Default Condition\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing curated dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Curated-default: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132/132 [03:07<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing pilot dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pilot-default: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 351/351 [07:45<00:00,  1.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Completed Default condition\n",
            "\n",
            "ðŸ“Š Processing with Feature-Preserving Condition\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing curated dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Curated-feature_preserving: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132/132 [03:12<00:00,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing pilot dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pilot-feature_preserving: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 351/351 [08:08<00:00,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Completed Feature-Preserving condition\n",
            "\n",
            "ðŸ“Š Processing with Legal-Context Condition\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing curated dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Curated-legal_context: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132/132 [03:03<00:00,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing pilot dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pilot-legal_context: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 351/351 [07:59<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Completed Legal-Context condition\n",
            "\n",
            "ðŸ“Š Processing with Bias-Aware Condition\n",
            "--------------------------------------------------\n",
            "\n",
            "Processing curated dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Curated-bias_aware: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 132/132 [03:13<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing pilot dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pilot-bias_aware: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 351/351 [08:05<00:00,  1.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Completed Bias-Aware condition\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}